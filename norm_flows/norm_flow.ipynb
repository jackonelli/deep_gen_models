{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalizing Flows - Home Assignment\n",
    "\n",
    "Welcome to this notebook assignment on normalizing flows, a powerful and expressive family of invertible generative models. The aim of this assignment is to provide you with a hands-on experience in understanding, implementing, and experimenting with normalizing flows using various techniques and architectures. We will start with simple 1D Gaussian distributions and progressively build up to more complex image data, such as the MNIST and CelebA datasets.\n",
    "\n",
    "Throughout this assignment, you will:\n",
    "\n",
    "1. Gain a solid understanding of the fundamental concepts behind normalizing flows, including probability density transformation and the change of variables formula.\n",
    "2. Implement an affine flow to model a 1D Gaussian distribution and learn about its limitations when dealing with more complex data.\n",
    "3. Explore the affine coupling flow and its ability to model a 2D Gaussian mixture distribution.\n",
    "4. Apply the lessons learned from the 1D and 2D examples to image data by implementing a RealNVP-like flow with more complex coupling networks and masking approaches.\n",
    "5. Learn about dequantization, squeeze, and split flows to improve the performance of your model on multiscale image data.\n",
    "6. Train your RealNVP-like flow on the MNIST and/or CelebA datasets and evaluate the quality of the generated samples.\n",
    "7. Perform interpolation and latent space manipulation to showcase the expressive power of normalizing flows.\n",
    "\n",
    "This assignment is designed to take 8-16 hours to complete, including time for implementing, training, and experimenting with your models. Most of this time should be spent on the image part, so don't get stuck on tuning the toy examples.\n",
    "\n",
    "By the end of this notebook assignment, you will have a deep understanding of normalizing flows and their practical applications, as well as the skills required to implement them in your future projects. Good luck!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conda environment\n",
    "You can create a GPU environment with the necessary packages using either one of these commands:\n",
    "- Option 1: conda create -n normflow_gpu python=3.9 pytorch torchvision torchaudio matplotlib ipykernel jupyter pandas pytorch-lightning pytorch-cuda=11.7 -c pytorch -c nvidia\n",
    "- Option 2: conda env create -f environments/normflow_gpu.yml\n",
    "\n",
    "We recommend that you use a GPU for this assignment, but if you don't have access to one, then you can create a CPU environment with the necessary packages using either one of these commands:\n",
    "- Option 1: conda create -n normflow_cpu python=3.9 pytorch torchvision torchaudio matplotlib ipykernel jupyter pandas pytorch-lightning cpuonly -c pytorch\n",
    "- Option 2: conda env create -f environments/normflow_cpu.yml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "from typing import Tuple, List, Set, Callable\n",
    "from functools import partial\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Distribution, Normal\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "from tests import *\n",
    "from utils import *\n",
    "\n",
    "device = (\n",
    "    torch.device(\"cpu\") if not torch.cuda.is_available() else torch.device(\"cuda:0\")\n",
    ")\n",
    "print(\"Using device\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flows and Normalizing Flows\n",
    "\n",
    "In this section, we introduce the basic building blocks for constructing normalizing flows. We provide skeleton classes for Flow, the base class for all flows, and NormalizingFlow, the base class for normalizing flows that combines multiple Flow instances with a base distribution.\n",
    "\n",
    "Recall that the primary goal of normalizing flows is to transform a simple base distribution (e.g., Gaussian) into a more complex distribution by learning a series of invertible transformations. The change of variables formula plays a crucial role in this process. The formula is as follows:\n",
    "\n",
    "$$\n",
    "p_X(x) = p_Z(f(x)) \\left|\\det\\frac{\\partial f(x)}{\\partial x}\\right|.\n",
    "$$\n",
    "\n",
    "The absolute value of the determinant of the Jacobian, $\\left|\\det\\frac{\\partial f(x)}{\\partial x}\\right|$, is essential as it accounts for the change in probability density due to the transformation.\n",
    "\n",
    "In the provided skeleton classes, you will find methods that need to be implemented, such as forward, inverse, and log_prob. Your task is to implement these methods while keeping the change of variables formula in mind. Once you complete the implementation, the test_normflow function will help you verify the correctness of your implementation. Good luck, and enjoy building your own normalizing flows!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Flow(nn.Module, ABC):\n",
    "    \"\"\"Base class for flows.\"\"\"\n",
    "\n",
    "    @abstractmethod\n",
    "    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"Run the flow in the normalizing direction (x -> z).\n",
    "        \n",
    "        Args:\n",
    "            x: Input tensor [B, ...]\n",
    "        Returns:\n",
    "            z: The transformed input. [B, ...]\n",
    "            log_det_jacobian: The log determinant of the jacobian. [B]\n",
    "        \"\"\"\n",
    "        \n",
    "\n",
    "    @abstractmethod\n",
    "    def inverse(self, z: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Run the flow in the inverse direction (z -> x).\n",
    "        \n",
    "        Args:\n",
    "            z: Input tensor [B, ...]\n",
    "        Returns:\n",
    "            x: The transformed input [B, ...]\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "class NormalizingFlow(Flow):\n",
    "    \"\"\"Base class for normalizing flows.\n",
    "    \n",
    "    A normalizing flow is a composition of flows, where the output of one flow is\n",
    "    the input of the next flow. The base distribution is the distribution that\n",
    "    the flow is trying to approximate.\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, flows: List[Flow], base_distribution: Distribution):\n",
    "        \"\"\"Initializes a flow with a given base distribution.\"\"\"\n",
    "        super().__init__()\n",
    "        # Note that the base distribution can be of lower dimensionality than the flow,\n",
    "        # e.g. a 1D base distribution for a 2D flow. This is useful to keep things\n",
    "        # simple when we move to higher dimensions, for example image flows.\n",
    "        self.base_distribution = base_distribution\n",
    "        assert len(flows) > 0, \"Need at least one flow.\"\n",
    "        self.flows = nn.ModuleList(flows)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"see Flow.forward\"\"\"\n",
    "        log_det_jacobian = torch.zeros(x.shape[0], device=x.device)\n",
    "        \n",
    "        x_t = x\n",
    "        for flow in self.flows:\n",
    "            x_t, tmp_jac = flow(x_t)\n",
    "            log_det_jacobian += tmp_jac\n",
    "        z = x_t\n",
    "        return z, log_det_jacobian\n",
    "\n",
    "    def inverse(self, z: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"see Flow.inverse\"\"\"\n",
    "        \n",
    "        x_t = z\n",
    "        for flow in self.flows:\n",
    "            x_t = flow.inverse(x_t)\n",
    "        x = x_t\n",
    "        return x\n",
    "\n",
    "    def log_prob(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Log probabilities of each sample in x.\n",
    "        \n",
    "        Args:\n",
    "            x: Input tensor [B, ...]\n",
    "        Returns:\n",
    "            log_prob: Log probabilities of each sample in x. [B]\n",
    "        \n",
    "        \"\"\"\n",
    "        z, log_det = self.forward(x)\n",
    "        z_log_prob = self.base_distribution.log_prob(z)\n",
    "        if len(z_log_prob.shape) > 1:\n",
    "            z_log_prob = z_log_prob.sum(list(range(1, len(z_log_prob.shape))))\n",
    "\n",
    "        log_prob = z_log_prob + log_det\n",
    "\n",
    "        return log_prob\n",
    "\n",
    "    def nll(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Negative log likelihood, averaged over all samples in the given batch.\"\"\"\n",
    "        return -self.log_prob(x).mean()\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def sample(self, shape: torch.Size) -> torch.Tensor:\n",
    "        \"\"\"Sample from the flow.\"\"\"\n",
    "        z = self.base_distribution.sample(shape)\n",
    "\n",
    "        x = self.inverse(z)\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "test_normflow(NormalizingFlow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1D toy example\n",
    "\n",
    "In this section, we will explore a simple one-dimensional normalizing flow using an affine transformation.\n",
    "\n",
    "We will start off by generating a 1D dataset from a non-standard Gaussian distribution. We will use a standard normal distribution as our base distribution for the normalizing flow, so we need to make sure that the target distrubution is at least a little bit different.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_gaussian(num_samples: int = 1000) -> np.ndarray:\n",
    "    return np.random.normal(loc=4, scale=2, size=num_samples)\n",
    "\n",
    "\n",
    "data_1d = torch.tensor(generate_gaussian(1000), dtype=torch.float32)\n",
    "vis_1d(data_1d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we provide a skeleton for the AffineFlow class, which inherits from the Flow class. This class performs an affine transformation on the input data, scaling and shifting it using learnable parameters. Your task is to implement the forward and inverse methods for this class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AffineFlow(Flow):\n",
    "    \"\"\"An affine flow.\"\"\"\n",
    "\n",
    "    def __init__(self, input_dim=1):\n",
    "        super().__init__()\n",
    "        self.scale = nn.Parameter(torch.ones(input_dim))\n",
    "        self.shift = nn.Parameter(torch.zeros(input_dim))\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"see Flow.forward\"\"\"\n",
    "        z = self.scale * x + self.shift\n",
    "        # Element-wise scaling => diagonal determinant\n",
    "        log_det_jacobian = torch.ones((z.size(0),)) * torch.log(torch.sum(self.scale))\n",
    "        return z, log_det_jacobian\n",
    "\n",
    "    def inverse(self, z: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"see Flow.inverse\"\"\"\n",
    "        x = (z - self.shift) / self.scale\n",
    "        return x\n",
    "\n",
    "# unittest by forward backward check\n",
    "testdata = torch.tensor(generate_gaussian(100), dtype=torch.float32)\n",
    "testflow = AffineFlow(1)\n",
    "test_flow(testflow, testdata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we have provided you with a training loop, your only task is to implement the loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_flow(\n",
    "    flow: NormalizingFlow,\n",
    "    data_func: Callable,\n",
    "    batch_size: int,\n",
    "    num_epochs: int,\n",
    "    vis_func: Callable,\n",
    "    vis_epochs: Set[int],\n",
    "    lr=1e-3,\n",
    "):\n",
    "    optimizer = torch.optim.Adam(flow.parameters(), lr=lr)\n",
    "    log_interval = num_epochs // 10\n",
    "    for epoch in range(num_epochs):\n",
    "        train_data = torch.tensor(data_func(batch_size), dtype=torch.float32)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loss = flow.nll(train_data)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if epoch % log_interval == 0:\n",
    "            print(f\"Epoch {epoch}/{num_epochs}, Loss: {loss.item()}\")\n",
    "        if vis_epochs and epoch in vis_epochs:\n",
    "            vis_func(flow=flow, epoch=epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, it's time to train your first flow! Instantiate your flow, and don't forget to pick a suitable base distribution, for instance a standard gaussian.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_distribution = Normal(loc=0, scale=1) # (hint: you can use Normal from torch.distributions)\n",
    "flow = NormalizingFlow(\n",
    "    flows=[AffineFlow(1)],\n",
    "    base_distribution=base_distribution,\n",
    ")\n",
    "\n",
    "num_epochs = 5000\n",
    "batch_size = 512\n",
    "vis_func = partial(vis_1d, data=data_1d, output_dir=\"output-1d\")\n",
    "vis_epochs = {*range(0, num_epochs, 100)} | {num_epochs - 1}\n",
    "train_flow(flow, generate_gaussian, batch_size, num_epochs, vis_func, vis_epochs)\n",
    "create_gif(output_dir=\"output-1d\", epochs=sorted(vis_epochs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expand to a 2d example\n",
    "\n",
    "In this section, we will expand our normalizing flow example to a 2D mixture of Gaussians with 4 modes. You might recognize this from the GAN assignment.\n",
    "\n",
    "First, let's create and visualize the 2D mixture of Gaussians with 4 modes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_mixture_of_gaussians(num_samples: int = 1000) -> np.ndarray:\n",
    "    # If you go outside -4,4 you will need to update the xlim and ylim arguments to vis_2d :)\n",
    "    means = [[-2, -2], [2, 2], [-2, 2], [2, -2]]\n",
    "    cov = np.eye(2) * 0.2\n",
    "    samples = []\n",
    "    for mean in means:\n",
    "        samples.append(\n",
    "            np.random.multivariate_normal(mean, cov, size=num_samples // len(means))\n",
    "        )\n",
    "    return np.vstack(samples)\n",
    "\n",
    "\n",
    "data_2d = torch.tensor(generate_mixture_of_gaussians(4000), dtype=torch.float32)\n",
    "vis_2d(data_2d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will attempt to model the 2D distribution using affine flows, as we did in the 1D example.\n",
    "\n",
    "However, this approach will not work well, as the affine transformation is not flexible enough to model the distribution. Later, we will construct a more complex normalizing flow to better model the distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flow = NormalizingFlow(\n",
    "    flows=[AffineFlow(2) for _ in range(4)],\n",
    "    base_distribution=base_distribution,  # reuse the 1D base distribution (but you could use MultivariateNormal here instead)\n",
    ")\n",
    "data_func = generate_mixture_of_gaussians\n",
    "\n",
    "num_epochs = 1000\n",
    "batch_size = 512\n",
    "vis_func = partial(vis_2d, data=data_2d, output_dir=\"output-2d-simple\")\n",
    "vis_epochs = {*range(0, num_epochs, 20)} | {num_epochs - 1}\n",
    "train_flow(flow, data_func, batch_size, num_epochs, vis_func, vis_epochs)\n",
    "create_gif(output_dir=\"output-2d-simple\", epochs=sorted(vis_epochs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coupling Flow\n",
    "\n",
    "Now we have seen that just stacking affine flows is not enough. So let us create a more complex transformation, namely a coupling affine flow.\n",
    "\n",
    "Quick recap:\n",
    "\n",
    "* Coupling flow: $y_1 = x_1$, $y_2 = h(x_2, \\theta(x_1))$\n",
    "* Affine transformation: $h(x) = \\mu + \\sigma x$\n",
    "* Coupling affine flow: $y_1 = x_1$, $y_2 = \\mu_2 + \\sigma_2 x_2$, where $(\\mu_2, \\sigma_2) = \\theta(x_1)$.\n",
    "* \\theta is typically a neural network.\n",
    "\n",
    "A few implementation tips:\n",
    "\n",
    "* a simple 2-layer MLP with ReLU activations should be enough. We have provided you with a simple MLP skeleton, but feel free to do something more complex if you want.\n",
    "* you can parametrize the scale as $\\log \\sigma_2 = a * tanh(MLP(x_1)) + b$, where $a$ and $b$ are learnable\n",
    "* pass the entire input to the MLP after applying a mask, instead of splitting the input into two parts\n",
    "* alternate which dimension you apply the coupling affine flow to (invert the masking)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    \"\"\"A simple multi-layer perceptron.\"\"\"\n",
    "\n",
    "    def __init__(self, input_size, n_hidden, hidden_size, output_size):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        \n",
    "        # Implement this\n",
    "\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "\n",
    "class AffineCouplingFlow(Flow):\n",
    "    def __init__(self, network: nn.Module, mask: torch.Tensor, in_channels: int = 1):\n",
    "        \"\"\"Coupling layer inside a normalizing flow.\"\"\"\n",
    "        super().__init__()\n",
    "        self.network = network  # Network should output 2 * in_channels\n",
    "        self.scale = nn.Parameter(torch.ones(in_channels), requires_grad=True)\n",
    "        self.scale_shift = nn.Parameter(torch.zeros(in_channels), requires_grad=True)\n",
    "        self.register_buffer(\"mask\", mask)\n",
    "\n",
    "    def _compute_coupling_parameters(self, x):\n",
    "        x = x * self.mask\n",
    "        log_s, t = self.network(x).chunk(2, dim=1)\n",
    "        scale = self.scale.view(1, -1, *([1] * (len(x.shape) - 2)))\n",
    "        scale_shift = self.scale_shift.view(1, -1, *([1] * (len(x.shape) - 2)))\n",
    "        \n",
    "        # Implement this (hint: use scale and scale_shift and check the equation in the hints above)\n",
    "        log_s = ... \n",
    "        \n",
    "        # Mask outputs (only transform the second part)\n",
    "        log_s = log_s * (1 - self.mask)\n",
    "        t = t * (1 - self.mask)\n",
    "        return log_s, t\n",
    "\n",
    "    def forward(self, x):\n",
    "        log_s, t = self._compute_coupling_parameters(x)\n",
    "\n",
    "        # Implement this\n",
    "        z = ...\n",
    "        log_det_jacobian = ...\n",
    "        \n",
    "        non_batch_dims = list(range(1, len(log_s.shape)))\n",
    "        log_det_jacobian = log_det_jacobian.sum(non_batch_dims)\n",
    "        return z, log_det_jacobian\n",
    "\n",
    "    def inverse(self, z):\n",
    "        log_s, t = self._compute_coupling_parameters(z)\n",
    "\n",
    "        # Implement this\n",
    "        x = ...\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "testdata = torch.tensor(generate_mixture_of_gaussians(100), dtype=torch.float32)\n",
    "testflow = AffineCouplingFlow(\n",
    "    network=MLP(2, 2, 32, 4),\n",
    "    mask=torch.tensor([1, 0], dtype=torch.float32),\n",
    "    in_channels=2,\n",
    ")\n",
    "test_flow(testflow, testdata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's train it! \n",
    "\n",
    "Note: Don't worry if your flow has some narrow lines connecting the modes, as long as it captures the general shape of the distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 2000\n",
    "batch_size = 512\n",
    "num_layers = 8\n",
    "flow = NormalizingFlow(\n",
    "    flows=[\n",
    "        AffineCouplingFlow(\n",
    "            network=MLP(2, 2, 64, 4),  # Feel free to change the network architecture\n",
    "            mask=torch.tensor(\n",
    "                [int(i % 2 == 0), int(i % 2 == 1)], dtype=torch.float32\n",
    "            ),  # Use alternating masks\n",
    "            in_channels=2,\n",
    "        )\n",
    "        for i in range(num_layers)\n",
    "    ],\n",
    "    base_distribution=base_distribution,  # reuse the 1D base distribution\n",
    ")\n",
    "\n",
    "vis_func = partial(vis_2d, data=data_2d, output_dir=\"output-2d-coupling\")\n",
    "# Extra many plots in the beginning since that it is where the evolution is most interesting\n",
    "vis_epochs = {*range(0, 500, 20)} | {*range(500, num_epochs, 50)} | {num_epochs - 1}\n",
    "vis_epochs = {epoch for epoch in vis_epochs if epoch < num_epochs}\n",
    "train_flow(flow, data_func, batch_size, num_epochs, vis_func, vis_epochs)\n",
    "create_gif(output_dir=\"output-2d-coupling\", epochs=sorted(vis_epochs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the learned distributions and generated samples look close to your target distribution, you have successfully implemented a coupling affine flow. Congratulations!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: Finally, let us reflect on the 2d mixture of Gaussians example. Why is the coupling affine flow able to model the distribution much better than a stack of affine flow? Also, how would you design a normalizing flow to model the 2d mixture of Gaussians if you were allowed to bake in prior knowledge about the distribution?\n",
    "\n",
    "===Your answer here==="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalizing Flows for Images\n",
    "\n",
    "Now that we have a good understanding of normalizing flows, let's apply them much more complex data - namely images.\n",
    "\n",
    "We will primarily use our MNIST-like downscaled CelebA dataset for this part of the assignment, but it's plug and play with mnist. You might want to perform the initial steps with MNIST, since samples from the CelebA dataset are more complex and it might be harder to get a good model. It might also be easier to build intuition with MNIST, since classes are so distinct. Nevertheless, MNIST is entirely optional, and you can just use CelebA from the beginning.\n",
    "\n",
    "Later on, after we have trained a good \"face flow\", we will experiment with interpolating and manipulating the latent space of the flow. This will allow us to generate new faces with different attributes, such as smiling, wearing glasses, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's begin by loading the dataset and visualizing some samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = \"celeba\"  # \"mnist\" or \"celeba\"\n",
    "train_set, train_data_loader, val_loader, test_loader = get_data(dataset)\n",
    "show_imgs([train_set[i][0] for i in range(60)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will introduce some more advanced flows, that are suitable for modeling images. This roughly follows the RealNVP paper, but we will not be implementing the full model exactly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dequantization\n",
    "\n",
    "A normalizing flow model provides an invertible mapping from the image space to the embedding space. However, images are often quantized as 8-bit unsigned integers with pixel values from 0 to 255. \n",
    "This is problematic because the model will learn to assign infinitely high probability to the integer values 0,1, ..., 255 (i.e. delta functions) and zero probability everywhere else.\n",
    "\n",
    "A solution is to stochastically smoothens out the input pixels by adding uniform random noise in the range [0,1) to each pixel.\n",
    "We will follow the strategy used in section 4.1 of the paper *Density Estimation Using Real-NVP* (https://arxiv.org/abs/1605.08803) and apply an additional transformation to the data after the addition of noise. \n",
    "This transformation maps the pixel values to a range that makes the network easier to train.\n",
    "\n",
    "The dequantization layer has already been implemented below, and should be used as the first layer in your flow model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dequantization(Flow):\n",
    "    def __init__(self, alpha=1e-5, quants=256):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "            alpha - small constant that is used to scale the original input.\n",
    "                    Prevents dealing with values very close to 0 and 1 when inverting the sigmoid\n",
    "            quants - Number of possible discrete values (usually 256 for 8-bit image)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        self.quants = quants\n",
    "\n",
    "    def forward(self, x):\n",
    "        z, ldj = self._dequant(x)\n",
    "        z, ldj = self._inverse_sigmoid(z, ldj)\n",
    "        return z, ldj\n",
    "\n",
    "    def inverse(self, z):\n",
    "        x = self._sigmoid(z)\n",
    "        x = x * self.quants\n",
    "        x = torch.floor(x).clamp(min=0, max=self.quants - 1).to(torch.int32)\n",
    "        return x\n",
    "\n",
    "    def _sigmoid(self, z):\n",
    "        z = torch.sigmoid(z)\n",
    "        z = (z - 0.5 * self.alpha) / (1 - self.alpha)\n",
    "        return z\n",
    "\n",
    "    def _inverse_sigmoid(self, z, ldj):\n",
    "        z = (\n",
    "            z * (1 - self.alpha) + 0.5 * self.alpha\n",
    "        )  # Scale to prevent boundaries 0 and 1\n",
    "        ldj += np.log(1 - self.alpha) * np.prod(z.shape[1:])\n",
    "        ldj += (-torch.log(z) - torch.log(1 - z)).sum(dim=[1, 2, 3])\n",
    "        z = torch.log(z) - torch.log(1 - z)\n",
    "        return z, ldj\n",
    "\n",
    "    def _dequant(self, x):\n",
    "        # Transform discrete values to continuous volumes\n",
    "        z = x.to(torch.float32)\n",
    "        z = z + torch.rand_like(z).detach()\n",
    "        z = z / self.quants\n",
    "        ldj = -np.log(self.quants) * np.prod(z.shape[1:])\n",
    "        return z, ldj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coupling Flow - revisited\n",
    "\n",
    "Here we can reuse the old affine coupling layer, but we need a different mask and a different coupling network, since the previous ones were only suitable for very simple data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we need to update the masking logic to work on image-like data. There are two common approaches to masking:\n",
    "* checkerboard masking - the mask alternates between 0 and 1 for each spatial dimension\n",
    "* channel masking - the mask alternates between 0 and 1 in the channel dimension\n",
    "\n",
    "We have provided both here, and a small test to check that they work as expected with your affine coupling layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_checkerboard_mask(h, w, invert=False):\n",
    "    x, y = torch.arange(h, dtype=torch.int32), torch.arange(w, dtype=torch.int32)\n",
    "    xx, yy = torch.meshgrid(x, y, indexing=\"ij\")\n",
    "    mask = torch.fmod(xx + yy, 2)\n",
    "    mask = mask.to(torch.float32).view(1, 1, h, w)\n",
    "    if invert:\n",
    "        mask = 1 - mask\n",
    "    return mask\n",
    "\n",
    "\n",
    "def create_channel_mask(c_in, invert=False):\n",
    "    mask = torch.cat(\n",
    "        [\n",
    "            torch.ones(c_in // 2, dtype=torch.float32),\n",
    "            torch.zeros(c_in - c_in // 2, dtype=torch.float32),\n",
    "        ]\n",
    "    )\n",
    "    mask = mask.view(1, c_in, 1, 1)\n",
    "    if invert:\n",
    "        mask = 1 - mask\n",
    "    return mask\n",
    "\n",
    "\n",
    "# Re-test the old coupling layer for image data. This should not fail, but if it does, you have to revisit the original implementation\n",
    "testflow = AffineCouplingFlow(\n",
    "    network=lambda x: torch.concat((x, x), dim=1),  # Dummy network with 2x out channels\n",
    "    mask=create_checkerboard_mask(32, 32, invert=True),\n",
    "    in_channels=4,\n",
    ")\n",
    "testdata = torch.rand(2, 4, 32, 32)\n",
    "test_flow(testflow, testdata, \"for image-like data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: Take a moment to reflect on the differences between the two approaches. Which one do you think will work better for images? Or perhaps we should use a combination of both? \n",
    "(visualizing the masks can help build some intuition)\n",
    "\n",
    "===Your answer here==="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coupling Network \n",
    "\n",
    "As mentioned earlier, the coupling network has no requirements on inversion, so we can use any network we want.\n",
    "\n",
    "We will give you some rough guidelines, but this part is mostly up to your imagination:\n",
    "* You can make assumptions on (and take advantage of) the spatial structure of the data, for example by using convolutional layers.\n",
    "* Start of with a relatively shallow network to keep the computational cost low.\n",
    "  * You should get okish results with ~5 layers, but you can go deeper and increase the quality\n",
    "  * Similarly, channel counts can be relatively low, e.g. 32 or 64\n",
    "* Stacking Resnet-style blocks is a good idea if you want to make a deeper coupling network "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    \"\"\"Example subnetwork for the coupling layer. \n",
    "    \n",
    "    Feel free to change the interface or discard this class entirely.\n",
    "    \"\"\"\n",
    "    def __init__(self, dim: int):\n",
    "        super().__init__()\n",
    "        # Implement this\n",
    "        self.net = ...\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Resnet-style forward pass.\"\"\"\n",
    "        # Feel free to change\n",
    "        return x + self.net(x)\n",
    "\n",
    "\n",
    "class CouplingNet(nn.Module):\n",
    "    \"\"\"Coupling network.\n",
    "    \n",
    "    This should be reasonably expressive, so we recommend building a network with multiple blocks.\n",
    "\n",
    "    Each block could be a resnet-style block, a gated convolution, or something else altogether.\n",
    "    \"\"\"\n",
    "    def __init__(self, c_in, c_hidden=64, num_layers=6):\n",
    "        super().__init__()\n",
    "        # Implement this (feel free to implement additional submodules as needed, e.g. the Block class)\n",
    "        self.net = ...\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Feel free to change\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "# Unit test of the old coupling layer\n",
    "testflow = AffineCouplingFlow(\n",
    "    network=CouplingNet(4),\n",
    "    mask=create_checkerboard_mask(32, 32, invert=True),\n",
    "    in_channels=4,\n",
    ")\n",
    "testdata = torch.rand(2, 4, 32, 32)\n",
    "test_flow(testflow, testdata, \"for image-like data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-scale image flow architecture\n",
    "\n",
    "To efficiently work with high-dimensional inputs, such as images, it's useful to have a way to downsample and upsample the data. A popular way to accomplish this for normalizing flows is to use a combination of squeeze and split flows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will use a squeeze flow to downsample the image spatially by a factor of 2, at the expense of quadrupling the number of channels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SqueezeFlow(Flow):\n",
    "    \"\"\"Squeeze flow. Shuffles data from spatial dimensions to channels.\"\"\"\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, C, H, W = x.shape\n",
    "        # Forward direction: H x W x C => H/2 x W/2 x 4C\n",
    "        z = x.reshape(B, C, H // 2, 2, W // 2, 2)\n",
    "        z = z.permute(0, 1, 3, 5, 2, 4)\n",
    "        z = z.reshape(B, 4 * C, H // 2, W // 2)\n",
    "        return z, torch.zeros(B, device=x.device)\n",
    "\n",
    "    def inverse(self, z):\n",
    "        B, C, H, W = z.shape\n",
    "        # Reverse direction: H/2 x W/2 x 4C => H x W x C\n",
    "        x = z.reshape(B, C // 4, 2, 2, H, W)\n",
    "        x = x.permute(0, 1, 4, 2, 5, 3)\n",
    "        x = x.reshape(B, C // 4, H * 2, W * 2)\n",
    "        return x\n",
    "\n",
    "\n",
    "test_input = torch.randn(1, 4, 32, 32)\n",
    "testflow = SqueezeFlow()\n",
    "test_flow(testflow, test_input)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we will reduce the number of channels using a split flow, which splits the channels into two parts and basically terminates the flow for one of the parts.\n",
    "When we want to run this splitting flow in reverse, we can simply sample the \"discarded\" part from our base distribution.\n",
    "\n",
    "Note that this implementation of the split flow is not invertible, because we do not store the discarded half. This is not a problem, because we will only use the split flow in the forward or backward direction, never both. A more general, but less practical, implementation would store the discarded half and then be invertible.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SplitFlow(Flow):\n",
    "    \"\"\"Split flow. Splits the input into two parts and discards one of them.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.base_distribution = Normal(loc=0.0, scale=1.0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        z, z_split = x.chunk(2, dim=1)\n",
    "        \n",
    "        # Implement this\n",
    "        # Since we are discarding z_split, we need to compute its log probability here, not at the end of all flows\n",
    "        log_prob_z_split = ... \n",
    "        \n",
    "        # As a little trick, we pass along the log probability of z_split as the log determinant of the Jacobian\n",
    "        ldj = log_prob_z_split\n",
    "        ldj = ldj.sum(list(range(1, len(ldj.shape))))\n",
    "        return z, ldj\n",
    "\n",
    "    def inverse(self, z):\n",
    "        z_split = self.base_distribution.sample(sample_shape=z.shape).to(device)\n",
    "        x = torch.cat([z, z_split], dim=1)\n",
    "        return x\n",
    "\n",
    "\n",
    "# We cannot test the forward-backward consistency of this flow because it is not invertible\n",
    "test_splitflow(SplitFlow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting it all together\n",
    "\n",
    "Now we can finally put all the pieces together and create a flow model that can properly model our celebrity faces.\n",
    "\n",
    "Here you will be tasked with implementing a multi-scale flow model, inspired by [RealNVP](https://arxiv.org/abs/1605.08803).\n",
    "\n",
    "To that end, implement the following flow model below:\n",
    "- start of with a Dequantization flow.\n",
    "- Add a couple (~2) affine coupling flows\n",
    "  - use your new CouplingNet\n",
    "  - and a checkerboard mask (remember to alternate the regular mask with the inverted one).\n",
    "- Add a SqueezeFlow (think about what this does to the dimensions)\n",
    "- Add a few more affine coupling flows (~4 or so)\n",
    "  - this time you can try the channel mask instead of the checkerboard mask. However, you can also alternate between the two.\n",
    "- Add both a SplitFlow and a SqueezeFlow (again think about dimensions)\n",
    "- Finally, add some more affine coupling flows (~4 or so)\n",
    "  - we recommend using the checkerboard mask here, but as always, feel free to experiment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_multiscale_flow() -> NormalizingFlow:\n",
    "    flows = []\n",
    "\n",
    "    # Implement this (according to the instructions above)\n",
    "    # hint: the initial input size is Bx1x28x28 (B is the batch size)\n",
    "\n",
    "    prior = Normal(torch.tensor(0.0, device=device), torch.tensor(1.0, device=device))\n",
    "    return NormalizingFlow(flows, prior)\n",
    "\n",
    "print_num_params(create_multiscale_flow())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might need to revisit the flow you just built, if you aren't getting good results. In that case we have some tips on what to focus on.\n",
    "\n",
    "Good candidates for tweaking:\n",
    "* architecture of the coupling network\n",
    "* number of flows in each scale\n",
    "* additional layers in each scale (e.g. ActNorm or Invertible1x1Conv from GLOW)\n",
    "* hidden channel count(s) in the coupling networks\n",
    "\n",
    "Not recommended for tweaking:\n",
    "* changing the number of scales\n",
    "* moving the dequantization layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the image flow\n",
    "\n",
    "We will train the flow using pytorch lightning. We have hidden away all the details inside lightning.py, but feel free to look and change the code if you want.\n",
    "\n",
    "Some guidelines:\n",
    "* The model should be able to learn something resembling faces after ~10 epochs\n",
    "* To get good looking faces you will need to train for a long time, e.g. 50-100 epochs (This is not required, but makes subsequent tasks more fun)\n",
    "* You can switch from CELEBA to MNIST by changing the `dataset` argument to `celeba` or `mnist` in the beginning of the image section. However, we have downscaled CELEBA to have the same resolution, number of channels and number of images as MNIST, so there are no speedups to be had by using MNIST.\n",
    "\n",
    "Practical tips:\n",
    "* you can pass additional arguments to `train_flow_lightning` and these will be passed to the `Trainer`. For example `limit_train_batches=5` for faster debugging.\n",
    "* you can modify other details by opening up `lightning.py` and changing the code there. For example, you can change the optimizer or the learning rate schedule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightning import train_flow_lightning\n",
    "\n",
    "assert (\n",
    "    device != \"cpu\"\n",
    "), \"Training on this part on CPU is slow! Uncomment if you want to try it anyway.\"\n",
    "\n",
    "max_epochs = 50  # 10 is enought to see some early indications, but 50 should give decent faces\n",
    "\n",
    "flow = create_multiscale_flow()\n",
    "model, metrics, trainer = train_flow_lightning(\n",
    "    flow, \"Flow Multiscale\", max_epochs, device, train_set, val_loader, test_loader\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "\n",
    "df = pandas.read_csv(trainer.logger.log_dir + \"/metrics.csv\")\n",
    "val_bpd = df[\"val_bpd\"].dropna().to_list()\n",
    "train_bpd = df[\"train_bpd\"].dropna().to_list()\n",
    "epoch = list(range(1, len(train_bpd) + 1))\n",
    "\n",
    "metrics_dict = {\"epoch\": epoch, \"train_bpd\": train_bpd, \"val_bpd\": val_bpd}\n",
    "df = pandas.DataFrame.from_dict(metrics_dict)\n",
    "df.set_index(\"epoch\").T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(20, 4))\n",
    "(line1,) = ax.plot(df[\"epoch\"], df[\"train_bpd\"], label=\"Train\", lw=2)\n",
    "(line2,) = ax.plot(df[\"epoch\"], df[\"val_bpd\"], label=\"Val\", lw=2)\n",
    "ax.legend()\n",
    "ax.set_xlabel(\"Epoch\")\n",
    "ax.set_ylabel(\"BPD\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl.seed_everything(42)\n",
    "num_samples = 16\n",
    "model = model.to(device)\n",
    "samples = model.flow.sample(shape=[num_samples, 8, 7, 7])\n",
    "show_imgs(samples.cpu())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Latent space interpolation and manipulation\n",
    "\n",
    "Apart from sampling we can also use the trained model to manipulate images in the latent space.\n",
    "We will do two small experiments in this section.\n",
    "\n",
    "1. Image interpolation\n",
    "2. adding and removing specific features from images\n",
    "\n",
    "Image interpolation can be performed by producing the latent representations of 2 images and then computing new latent representations, which are convex combination of the two original latent representations.\n",
    "These new latent representations can then be transformed back into image space using the inverse flow.\n",
    "If everything works the resulting images should look like a gradual transition from one face to another. \n",
    "\n",
    "\n",
    "**Question:** If we had done interpolation directly in the image space instead, then the result would have looked much worse. Why is that?\n",
    "\n",
    "===Your answer here==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def interpolate(model, img1, img2, num_steps=16):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "        model - object of ImageFlow class that represents the (trained) flow model\n",
    "        img1, img2 - Image tensors of shape [1, 28, 28]. Images between which should be interpolated.\n",
    "        num_steps - Number of interpolation steps. 8 interpolation steps mean 6 intermediate pictures besides img1 and img2\n",
    "    \"\"\"\n",
    "    imgs = torch.stack([img1, img2], dim=0).to(model.device)\n",
    "    z, _ = model.flow(imgs)\n",
    "    alpha = torch.linspace(0, 1, steps=num_steps, device=z.device).view(-1, 1, 1, 1)\n",
    "    interpolations = z[0:1] * alpha + z[1:2] * (1 - alpha)\n",
    "    interp_imgs = model.flow.inverse(interpolations)\n",
    "    show_imgs(interp_imgs, row_size=num_steps)\n",
    "\n",
    "\n",
    "exmp_imgs, _ = next(iter(train_data_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl.seed_everything(42)\n",
    "for i in range(6):\n",
    "    interpolate(model, exmp_imgs[2 * i], exmp_imgs[2 * i + 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The CELEBA dataset has a large number of categorical labels/attributes, which are listed in the dictionary below (along with the index corresponding index in the datasets list of attributes).\n",
    "\n",
    "Since we are working with a highly down-scaled version of CELEBA not all of these attributes will be very useful. For example \"Bags_Under_Eyes\" is not so useful due to the poor resolution of the image, but \"Eyeglasses\" is still very useful.\n",
    "\n",
    "We start off by computing two lists containing arrays of all the positive and negative examples of each of the 40 attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attr2idx_dict = {\n",
    "    \"5_o_Clock_Shadow\": 0,\n",
    "    \"Arched_Eyebrows\": 1,\n",
    "    \"Attractive\": 2,\n",
    "    \"Bags_Under_Eyes\": 3,\n",
    "    \"Bald\": 4,\n",
    "    \"Bangs\": 5,\n",
    "    \"Big_Lips\": 6,\n",
    "    \"Big_Nose\": 7,\n",
    "    \"Black_Hair\": 8,\n",
    "    \"Blond_Hair\": 9,\n",
    "    \"Blurry\": 10,\n",
    "    \"Brown_Hair\": 11,\n",
    "    \"Bushy_Eyebrows\": 12,\n",
    "    \"Chubby\": 13,\n",
    "    \"Double_Chin\": 14,\n",
    "    \"Eyeglasses\": 15,\n",
    "    \"Goatee\": 16,\n",
    "    \"Gray_Hair\": 17,\n",
    "    \"Heavy_Makeup\": 18,\n",
    "    \"High_Cheekbones\": 19,\n",
    "    \"Male\": 20,\n",
    "    \"Mouth_Slightly_Open\": 21,\n",
    "    \"Mustache\": 22,\n",
    "    \"Narrow_Eyes\": 23,\n",
    "    \"No_Beard\": 24,\n",
    "    \"Oval_Face\": 25,\n",
    "    \"Pale_Skin\": 26,\n",
    "    \"Pointy_Nose\": 27,\n",
    "    \"Receding_Hairline\": 28,\n",
    "    \"Rosy_Cheeks\": 29,\n",
    "    \"Sideburns\": 30,\n",
    "    \"Smiling\": 31,\n",
    "    \"Straight_Hair\": 32,\n",
    "    \"Wavy_Hair\": 33,\n",
    "    \"Wearing_Earrings\": 34,\n",
    "    \"Wearing_Hat\": 35,\n",
    "    \"Wearing_Lipstick\": 36,\n",
    "    \"Wearing_Necklace\": 37,\n",
    "    \"Wearing_Necktie\": 38,\n",
    "    \"Young\": 39,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attr_pos_indices = [train_set.tensors[1][:, i] == True for i in range(0, 40)]\n",
    "pos_images = [train_set.tensors[0][attr_pos_indices[i], :, :, :] for i in range(0, 40)]\n",
    "\n",
    "attr_neg_indices = [train_set.tensors[1][:, i] == False for i in range(0, 40)]\n",
    "neg_images = [train_set.tensors[0][attr_neg_indices[i], :, :, :] for i in range(0, 40)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function modify_img takes an image $x$, an attribute of interest (e.g. \"Eyeglasses\") and the arrays of positive and negative examples defined above, and modifies the image $x$ to move towards the positive examples.\n",
    "\n",
    "In order to do this we first compute latent representations of a subset of the positive and negative examples. We used two hundred, but feel free to adjust it if you run into GPU memory issues.\n",
    "\n",
    "Next we take the mean of these vectors and subtract one from the other, i.e. $delta = z_{pos} - z_{neg}$.\n",
    "\n",
    "This gives us a direction to move in in the latent space, in order to modify the embedding z of the input image x.\n",
    "\n",
    "In practice it may be easier to modify images if you also normalize, so delta becomes $delta \\gets delta\\frac{||z||_2}{||delta||_2}$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def modify_img(model, x, attribute, gain, pos_images, neg_images, num_steps=16):\n",
    "    class_idx = attr2idx_dict[attribute]\n",
    "\n",
    "    x_pos = pos_images[class_idx][\n",
    "        0 : np.minimum(200, len(pos_images[class_idx])), :, :, :\n",
    "    ].to(model.device)\n",
    "    x_neg = neg_images[class_idx][\n",
    "        0 : np.minimum(200, len(neg_images[class_idx])), :, :, :\n",
    "    ].to(model.device)\n",
    "\n",
    "    z, _ = model.flow(x)\n",
    "    z_pos, _ = model.flow(x_pos)\n",
    "    z_neg, _ = model.flow(x_neg)\n",
    "    delta = z_pos.mean(axis=0) - z_neg.mean(axis=0)\n",
    "\n",
    "    norm_z = z.norm()\n",
    "    norm_delta = delta.norm()\n",
    "    delta = gain * delta * norm_z / norm_delta\n",
    "    alpha = torch.linspace(0, 1.0, steps=num_steps, device=z.device).view(-1, 1, 1, 1)\n",
    "    z_altered = z + alpha * delta\n",
    "\n",
    "    x_altered = model.flow.inverse(z_altered)\n",
    "\n",
    "    notstring = \"not \" * (gain < 0)\n",
    "    show_imgs(\n",
    "        x_altered,\n",
    "        row_size=num_steps,\n",
    "        title='Increasing \"' + notstring + attribute + '\"-ness-->',\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following cell to plot some modified images.\n",
    "\n",
    "Note that sign of the gain controls whether we are trying to add or remove a feature.\n",
    "\n",
    "**Question:** Try adding and removing different features by changing the arguments to the cell below. What do you observe? What does this experiment tell you about the latent space?\n",
    "\n",
    "===Your answer here==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = exmp_imgs[6].to(model.device).unsqueeze(axis=0)\n",
    "x2 = exmp_imgs[25].to(model.device).unsqueeze(axis=0)\n",
    "\n",
    "# List of gain factors and attributes for image x1\n",
    "gain_list_1 = # Implement this (a list of floats)\n",
    "attr_list_1 = # Implement this (a list of strings corresponding to the attributes you want to add or remove)\n",
    "\n",
    "# List of gain factors and attributes for image x2\n",
    "gain_list_2 = # Implement this\n",
    "attr_list_2 = # Implement this\n",
    "\n",
    "# Modify image x1\n",
    "for (gain, attr) in zip(gain_list_1, attr_list_1):\n",
    "    modify_img(model, x1, attr, gain, pos_images, neg_images, num_steps=8)\n",
    "\n",
    "# Modify image x2\n",
    "for (gain, attr) in zip(gain_list_2, attr_list_2):\n",
    "    modify_img(model, x2, attr, gain, pos_images, neg_images, num_steps=8)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we use the learned model to modify an image of Zlatan, but since our model was trained on 28x28 images we need to downscale the image first. \n",
    "\n",
    "If you want to make things extra fun, you can put in your own portrait here instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "img_file = \"zlatan.jpeg\"  # Replace with your own face (optional) \n",
    "\n",
    "test_img = Image.open(img_file).convert(\"L\")\n",
    "width, height = test_img.width, test_img.height\n",
    "delta_w = (np.abs(width - height) // 2) * (width > height)\n",
    "delta_h = (np.abs(width - height) // 2) * (width <= height)\n",
    "\n",
    "test_img = test_img.crop((delta_w, 0, width - delta_w, height))\n",
    "test_img = test_img.resize((28, 28))\n",
    "test_img = (\n",
    "    torch.tensor(np.stack((test_img,), axis=-1))\n",
    "    .squeeze(-1)\n",
    "    .unsqueeze(0)\n",
    "    .unsqueeze(0)\n",
    "    .to(model.device)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gain_list = # Implement this\n",
    "attr_list = # Implement this\n",
    "\n",
    "for (gain, attr) in zip(gain_list, attr_list):\n",
    "    modify_img(model, test_img, attr, gain, pos_images, neg_images, num_steps=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final reflections\n",
    "**Question:** What are the advantages and disadvantages of normalizing flows compared to GANs and VAEs?\n",
    "\n",
    "===YOUR ANSWER HERE==="
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "normflow_gpu",
   "language": "python",
   "name": "normflow_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
