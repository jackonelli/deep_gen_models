{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GANs\n",
    "In this assignment, you will first implement the original GAN and the Wasserstein GAN (WGAN) on a toy problem to see how the relatively small changes can lead to big changes during training. Then, you will train a Conditional GAN (cGAN) on the same dataset as in the previos assignment, namely MNIST.\n",
    "\n",
    "## Setup\n",
    "To facilitate the assignment, we use the same enviorments as in the previous assignments. If you installed the environment in the previous assignment, you can simply do `conda activate vae`. Otherwise, run the following:\n",
    "```\n",
    "conda env create -f environments/environment-gpu.yml\n",
    "conda activate vae\n",
    "```\n",
    "\n",
    "or\n",
    "\n",
    "```\n",
    "conda env create -f environments/environment-cpu-only.yml\n",
    "conda activate vae\n",
    "```\n",
    "\n",
    "## How to complete this assignment\n",
    "\n",
    "Throughout this assignment there are several places where you will need to fill in code. These are marked with `YOUR CODE HERE` comments. Furthermore, there are several places where you will need to answer questions. These are marked with `YOUR ANSWER HERE` comments. You should replace the `YOUR CODE HERE` and `YOUR ANSWER HERE` comments with your code and answers. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ca19f7a80c1f0e9c7f88eb45d52ba0e4",
     "grade": false,
     "grade_id": "cell-a4354e938bbbbcb7",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import List, Tuple\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from utils import init_params, print_params, clamp_params, plot_inline\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "np.random.seed(1337)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "031f1c57921bcc9606845a5ddd534ba6",
     "grade": false,
     "grade_id": "cell-f90f0d5f184b0b83",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Part 1: Original GAN vs WGAN\n",
    "\n",
    "In this part of the assignment, you will implement a simple GAN to learn the distribution of a 2D toy-dataset. You will implement the trainig loop and the targets for the critic and the generator, **both according to the original GAN paper and the WGAN paper**. Here, we want you to see that WGANs are more stable and produce better results. \n",
    "\n",
    "### Dataset\n",
    "The dataset is a simple Gaussian mixture with 4 components located at (-1, -1), (-1, 1), (1, 1), and (1, -1). The code below will load the dataset and plot it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "08017243fa6289f80f7b0f8a709dbc54",
     "grade": false,
     "grade_id": "cell-ddf9e76da53c7533",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class GaussianMixtureDataset(Dataset):\n",
    "    def __init__(self):\n",
    "        self.probs = [0.25, 0.25, 0.25, 0.25]\n",
    "        self.sigmas_x = [0.05, 0.05, 0.05, 0.05]\n",
    "        self.sigmas_y = [0.05, 0.05, 0.05, 0.05]\n",
    "        self.means_x = [-1, 1, 1, -1]\n",
    "        self.means_y = [-1, -1, 1, 1]\n",
    "\n",
    "        self.examples: np.ndarray = self.make_examples()  # Nx2 array\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return self.examples.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx: np.ndarray) -> np.ndarray:\n",
    "        return self.examples[idx, :]\n",
    "\n",
    "    # Return one sample from a 2D gaussian mixture model\n",
    "    def get_gmm_sample(\n",
    "        self,\n",
    "        probs: List[float],\n",
    "        means_x: List[float],\n",
    "        means_y: List[float],\n",
    "        sigmas_x: List[float],\n",
    "        sigmas_y: List[float],\n",
    "    ) -> Tuple[float, float]:\n",
    "        nof_gaussians: int = len(means_x)\n",
    "        gaussian_no: int = np.random.choice(np.arange(nof_gaussians), p=probs)\n",
    "        value_x: float = np.random.normal(means_x[gaussian_no], sigmas_x[gaussian_no])\n",
    "        value_y: float = np.random.normal(means_y[gaussian_no], sigmas_y[gaussian_no])\n",
    "        return (value_x, value_y)\n",
    "\n",
    "    # Create all examples\n",
    "    def make_examples(self) -> np.ndarray:\n",
    "        n_examples: int = 10_000\n",
    "        examples: np.ndarray = np.zeros((n_examples, 2), np.float32)\n",
    "        for ix in range(n_examples):\n",
    "            (x, y) = self.get_gmm_sample(\n",
    "                self.probs, self.means_x, self.means_y, self.sigmas_x, self.sigmas_y\n",
    "            )\n",
    "            examples[ix, 0] = x\n",
    "            examples[ix, 1] = y\n",
    "\n",
    "        return examples\n",
    "\n",
    "\n",
    "dataset = GaussianMixtureDataset()\n",
    "dataset.make_examples()\n",
    "fig, ax = plt.subplots(figsize=(6, 6))\n",
    "ax.scatter(dataset.examples[:, 0], dataset.examples[:, 1], s=1)\n",
    "plt.title(\"Gaussian Mixture Dataset\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "67222322fc28daf00dfa24b8bb0a1770",
     "grade": false,
     "grade_id": "cell-e6b9f7bfaf9ef8d5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Generator and Critic\n",
    "Here, we want you to implement the generator and the critic. Both should be simple MLPs. \n",
    "\n",
    "The generator should be an MLP with one input layer, one hidden layer with 128 units, and one output layer with two units and no activation. The other layers should have a LeakyReLU activation.\n",
    "\n",
    "The critic should be an MLP with one input layer, one hidden layer with 8 units, and one output layer with 2 units and no activation. The other layers should have a LeakyReLU activation.\n",
    "\n",
    "*If you want to use a different architecture, you can do so, but you will have to tune the hyperparameters yourself.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8ea2d9e773d01a828faf350534175fca",
     "grade": true,
     "grade_id": "cell-96a2afc23e17f14f",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Generator model, generating examples from noise\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, latent_space_dims: int):\n",
    "        super(Generator, self).__init__()\n",
    "        self.latent_space_dims = latent_space_dims\n",
    "        # Define the generator network.\n",
    "        # The input is an NxL tensor with N examples drawn from an\n",
    "        # L-dimensional drawn normal distribution. The output should be an Nx2\n",
    "        # tensor containing N generated examples.\n",
    "        num_hidden = 128\n",
    "        self.fc = nn.Linear(self.latent_space_dims, num_hidden)\n",
    "        self.out = nn.Linear(num_hidden, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # return self.net(x)\n",
    "        h = F.leaky_relu(self.fc(x))\n",
    "        h = self.out(h)\n",
    "        return h\n",
    "\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Critic, self).__init__()\n",
    "        # Define the critic network.\n",
    "        # The input is an Nx2 tensor with N examples. The output should be\n",
    "        # classification results (logits) arranged as an Nx2 tensor.\n",
    "        num_hidden = 8\n",
    "        self.fc = nn.Linear(2, num_hidden)\n",
    "        self.fc_extra = nn.Linear(num_hidden, num_hidden)\n",
    "        self.out = nn.Linear(num_hidden, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # return self.net(x)\n",
    "        h = F.leaky_relu(self.fc(x))\n",
    "        h = F.leaky_relu(self.fc_extra(h))\n",
    "        h = self.out(h)\n",
    "        return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ff3a9cc43a6bd73c686dc2e931e4c7ae",
     "grade": false,
     "grade_id": "cell-94e8f61d829a4a74",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "lat_dim = 42\n",
    "generator_input = torch.randn(10, lat_dim)\n",
    "generator = Generator(latent_space_dims=lat_dim)\n",
    "assert generator(generator_input).shape == (10, 2), \"Generator output shape is wrong\"\n",
    "\n",
    "critic_input = torch.randn(10, 2)\n",
    "critic = Critic()\n",
    "assert critic(critic_input).shape == (10, 2), \"Critic output shape is wrong\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6bce82c2af0fd8c7ae55b61d33e2da94",
     "grade": false,
     "grade_id": "cell-5089718313d27986",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Targets\n",
    "Here, we want you to implement the GAN target for the generator and the critic. We want to implement the original GAN target and the WGAN target, **however**, we want you to start with only implementing the `original` targets and then continue to training the networks. Once that is done, you can come back here and implement the `wgan` targets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5fd057af046c615db390032298eef65a",
     "grade": true,
     "grade_id": "cell-bf07affd22f7928c",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class GeneratorTarget(nn.Module):\n",
    "    def __init__(self, gan_type: str):\n",
    "        if gan_type not in (\"original\", \"wgan\"):\n",
    "            raise ValueError(\"GAN type should be either 'original' or 'wgan'.\")\n",
    "        self.gan_type = gan_type\n",
    "        super(GeneratorTarget, self).__init__()\n",
    "\n",
    "    def forward(self, critic_output_fake: torch.Tensor):\n",
    "        \"\"\"Compute the target for the generator.\n",
    "\n",
    "        Args:\n",
    "            critic_output_fake: Output of the critic for the fake examples. Batch size x 2.\n",
    "\n",
    "        \"\"\"\n",
    "        if self.gan_type == \"original\":\n",
    "            # Compute prob vector from logits\n",
    "            prob = F.softmax(critic_output_fake, dim=1)\n",
    "            # Extract p(G(z) = real) (arbitrary choose first column as prob of real)\n",
    "            prob_real = prob[:, 0]\n",
    "            target = torch.mean(torch.log(1 - prob_real))\n",
    "            \n",
    "\n",
    "        elif self.gan_type == \"wgan\":\n",
    "            # Compute prob vector from logits\n",
    "            prob = F.softmax(critic_output_fake, dim=1)\n",
    "            # Extract p(G(z) = real) (arbitrary choose first column as prob of real)\n",
    "            prob_real = prob[:, 0]\n",
    "            target = - torch.mean(torch.log(prob_real))\n",
    "\n",
    "        else:\n",
    "            raise ValueError(\"GAN type should be either 'original' or 'wgan'.\")\n",
    "\n",
    "        return target\n",
    "\n",
    "# l_g = GeneratorTarget(\"original\")\n",
    "# d_g_z = torch.randn((10, 2))\n",
    "# l_g(d_g_z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "cc8f6a86cba32e833968df74bda5bf28",
     "grade": true,
     "grade_id": "cell-f273a53d20c4477f",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class CriticTarget(nn.Module):\n",
    "    def __init__(self, gan_type):\n",
    "        if gan_type not in (\"original\", \"wgan\"):\n",
    "            raise ValueError(\"Mode should be either 'original' or 'wgan'.\")\n",
    "        self.gan_type = gan_type\n",
    "        super(CriticTarget, self).__init__()\n",
    "\n",
    "    def forward(\n",
    "        self, critic_output_real: torch.Tensor, critic_output_fake: torch.Tensor\n",
    "    ):\n",
    "        \"\"\"Compute the target for the critic.\n",
    "\n",
    "        Args:\n",
    "            critic_output_real: Output of the critic for real examples.\n",
    "            critic_output_fake: Output of the critic for fake examples.\n",
    "        Returns:\n",
    "            Target for the critic.\n",
    "        \"\"\"\n",
    "        if self.gan_type == \"original\":\n",
    "            # Compute prob vector from logits\n",
    "            # Extract p(G(z) = real) (arbitrary choose first column as prob of real)\n",
    "            prob_real = F.softmax(critic_output_real, dim=1)[:, 0]\n",
    "            ll_real = torch.mean(torch.log(prob_real))\n",
    "            \n",
    "            # Compute prob vector from logits\n",
    "            # Extract p(G(z) = real) (arbitrary choose first column as prob of real)\n",
    "            prob_fake = F.softmax(critic_output_fake, dim=1)[:, 0]\n",
    "            ll_fake = torch.mean(torch.log(1 - prob_fake))\n",
    "\n",
    "            # Note: Maximising this target.\n",
    "            target = ll_real + ll_fake\n",
    "\n",
    "        elif self.gan_type == \"wgan\":\n",
    "            # Compute prob vector from logits\n",
    "            # Extract p(G(z) = real) (arbitrary choose first column as prob of real)\n",
    "            prob_real = F.softmax(critic_output_real, dim=1)[:, 0]\n",
    "            ll_real = torch.mean(torch.log(prob_real))\n",
    "            \n",
    "            # Compute prob vector from logits\n",
    "            # Extract p(G(z) = real) (arbitrary choose first column as prob of real)\n",
    "            prob_fake = F.softmax(critic_output_fake, dim=1)[:, 0]\n",
    "            ll_fake = torch.mean(torch.log(prob_fake))\n",
    "\n",
    "            # Note: Maximising this target.\n",
    "            target = ll_real - ll_fake\n",
    "        else:\n",
    "            raise ValueError(\"GAN type should be either 'original' or 'wgan'.\")\n",
    "\n",
    "        return target\n",
    "\n",
    "l_d = CriticTarget(\"original\")\n",
    "d_g_z = torch.randn((10, 2))\n",
    "d_x = torch.randn((8, 2))\n",
    "l_d(d_x, d_g_z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4f4b5cb48610475f742c9ad7ce6072de",
     "grade": false,
     "grade_id": "cell-9c5b7ae47f0d3beb",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Trainer\n",
    "Here, we want you to implement the training of the GAN. Most of the code is written but you should complete the `train_generator`, and the `train_critic` methods.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e141253ae51187e83a844fb944c3583e",
     "grade": true,
     "grade_id": "cell-dbfe2cee7d135185",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class GanTrainer:\n",
    "    def __init__(\n",
    "        self,\n",
    "        mode: str,\n",
    "        latent_size: int,\n",
    "        learning_rate: float,\n",
    "        n_generator_steps: int,\n",
    "        n_critics_steps: int,\n",
    "        batch_size: int,\n",
    "        training_steps: int,\n",
    "        device: str,\n",
    "    ):\n",
    "        assert mode in (\n",
    "            \"original\",\n",
    "            \"wgan\",\n",
    "        ), \"Mode should be either 'original' or 'wgan'.\"\n",
    "\n",
    "        self.mode = mode\n",
    "        self.device = device\n",
    "        self.batch_size = batch_size\n",
    "        self.latent_size = latent_size\n",
    "        self.n_critics_steps = n_critics_steps\n",
    "        self.n_generator_steps = n_generator_steps\n",
    "        self.lr = learning_rate\n",
    "        self.training_steps = training_steps\n",
    "\n",
    "        # Setup data\n",
    "        self.dataset = GaussianMixtureDataset()\n",
    "        self.data_loader = DataLoader(self.dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "        # Setup models\n",
    "        self.generator = Generator(latent_size)\n",
    "        self.critic = Critic()\n",
    "        self.critic.apply(init_params)\n",
    "\n",
    "        # Setup optimization targets\n",
    "        self.generator_target = GeneratorTarget(self.mode)\n",
    "        self.critic_target = CriticTarget(self.mode)\n",
    "\n",
    "        # Setup optimizers, note that these only optimize the parameters of their respective models\n",
    "        self.critic_optim = torch.optim.RMSprop(\n",
    "            self.critic.parameters(), maximize=True, lr=self.lr\n",
    "        )\n",
    "        self.generator_optim = torch.optim.RMSprop(\n",
    "            self.generator.parameters(), maximize=False, lr=self.lr\n",
    "        )\n",
    "\n",
    "        # Move everything to the right device (CPU or GPU)\n",
    "        self.generator.to(self.device)\n",
    "        self.critic.to(self.device)\n",
    "        self.generator_target.to(self.device)\n",
    "        self.critic_target.to(self.device)\n",
    "\n",
    "    # Run main training loop\n",
    "    def train(self):\n",
    "        critic_targets = []\n",
    "        generator_targets = []\n",
    "        for step in range(self.training_steps):\n",
    "            # Train critic and generator for a few steps each\n",
    "            critic_target = self.train_critic()\n",
    "            generator_target = self.train_generator()\n",
    "\n",
    "            # Show progress\n",
    "            if step % 500 == 0:\n",
    "                # print_params(self.critic)  # Enable for debugging\n",
    "                generator_targets.append(generator_target)\n",
    "                critic_targets.append(critic_target)\n",
    "                # plot the generated samples\n",
    "                noise = self.make_noise_input(512)\n",
    "                examples = self.generator(noise)\n",
    "                examples = examples.detach().cpu().numpy()\n",
    "                plot_inline(\n",
    "                    examples,\n",
    "                    generator_targets,\n",
    "                    critic_targets,\n",
    "                    step,\n",
    "                    self.dataset.means_x,\n",
    "                    self.dataset.means_y,\n",
    "                )\n",
    "                print(\n",
    "                    f\"step: {step}, critic target (max): {critic_target}, generator target (min): {generator_target}\"\n",
    "                )\n",
    "\n",
    "        return self.generator, critic_targets, generator_targets\n",
    "\n",
    "    # Generate an NxL tensor of noise inputs to the generator\n",
    "    # (N is sample size, L is latent space dimensionality)\n",
    "    def make_noise_input(self, sample_size: int):\n",
    "        return torch.randn(sample_size, self.latent_size).to(self.device)\n",
    "\n",
    "    # Run a few steps of critic training\n",
    "    def train_critic(self):\n",
    "        \"\"\"Train the critic for a few steps.\"\"\"\n",
    "        for _ in range(self.n_critics_steps):\n",
    "            real_data = next(iter(self.data_loader)).to(self.device)\n",
    "            z = self.make_noise_input(self.batch_size).to(self.device)\n",
    "            fake_data = self.generator(z)\n",
    "            real_logits, fake_logits = self.critic(real_data), self.critic(fake_data)\n",
    "            loss = self.critic_target(real_logits, fake_logits)\n",
    "            self.critic_optim.zero_grad()\n",
    "            loss.backward()\n",
    "            self.critic_optim.step()\n",
    "\n",
    "            if self.mode == \"wgan\":\n",
    "                with torch.no_grad():\n",
    "                    self.critic.apply(clamp_params)\n",
    "\n",
    "        # Return the last loss we observed\n",
    "        return loss.item()\n",
    "\n",
    "    # Run a few steps of generator training\n",
    "    def train_generator(self):\n",
    "        \"\"\"Train the generator for a few steps.\"\"\"\n",
    "        for _ in range(self.n_generator_steps):\n",
    "            z = self.make_noise_input(self.batch_size).to(self.device)\n",
    "            fake_data = self.generator(z)\n",
    "            fake_logits = self.critic(fake_data)\n",
    "            loss = self.generator_target(fake_logits)\n",
    "            # note that the generator optimizer only has access to the generators' parameters\n",
    "            # thus we don't need to zero the critic parameters\n",
    "            self.generator_optim.zero_grad()\n",
    "            loss.backward()\n",
    "            self.generator_optim.step()\n",
    "\n",
    "        # Return the last loss we observed\n",
    "        return loss.item()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "aba4e4ffb501b5be232f9fd7a347bec4",
     "grade": false,
     "grade_id": "cell-c77dbe6224cf3cd5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "First, implement the original GAN loss for the critic and the generator, and train the GAN by having `GAN_TYPE = \"original\"`. With the parameter settings below, you should see that the training results in mode collapse. After that, implement the WGAN loss for the critic and the generator, and train the GAN by having `GAN_TYPE = \"wgan\"`. With the parameter settings below, you should see that the training results in a much more stable training process and produces better results.\n",
    "\n",
    "You can also play around with the parameters and see if you can stabilize the training of the original GAN. You can also try to improve the results of the WGAN by changing the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "# Training details, default:\n",
    "# 10_000 steps, 256 batch size, 32 latent space dimensions, 1 generator step, 5 critic steps, 0.0005 learning rate\n",
    "BATCH_SIZE = 256\n",
    "LATENT_SPACE_DIMS = 32\n",
    "GENERATOR_TRAINING_STEPS = 1\n",
    "CRITIC_TRAINING_STEPS = 5\n",
    "LEARNING_RATE = 0.0005\n",
    "TRAINING_STEPS = 10_000\n",
    "OUTPUT_DIR = \"./output\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# WGAN specifics\n",
    "PARAM_CLAMP = 0.01\n",
    "\n",
    "# original or wgan\n",
    "GAN_TYPE = \"wgan\"\n",
    "\n",
    "trainer = GanTrainer(\n",
    "    mode=GAN_TYPE,\n",
    "    latent_size=LATENT_SPACE_DIMS,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    n_generator_steps=GENERATOR_TRAINING_STEPS,\n",
    "    n_critics_steps=CRITIC_TRAINING_STEPS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    training_steps=TRAINING_STEPS,\n",
    "    device=DEVICE,\n",
    ")\n",
    "\n",
    "generator, critic_targets, generator_targets = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generator, critic_targets, generator_targets = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "def save(trainer, label, type_):    \n",
    "    gen_name = f\"models/{type_}_gen_{label}.pth\"\n",
    "    torch.save(trainer.generator.state_dict(), gen_name)\n",
    "    crit_name = f\"models/{type_}_crit_{label}.pth\"\n",
    "    torch.save(trainer.critic.state_dict(), crit_name)\n",
    "\n",
    "def load(label, type_):\n",
    "    gen_name = f\"models/{type_}_gen_{label}.pth\"\n",
    "    gen = Generator(LATENT_SPACE_DIMS)\n",
    "    gen.load_state_dict(torch.load(gen_name))\n",
    "    gen.to(DEVICE)\n",
    "    \n",
    "    crit_name = f\"models/{type_}_crit_{label}.pth\"\n",
    "    crit = Critic()\n",
    "    crit.load_state_dict(torch.load(crit_name))\n",
    "    crit.to(DEVICE)\n",
    "    return gen, crit\n",
    "\n",
    "label = \"cross\"\n",
    "# save(trainer, label, GAN_TYPE)\n",
    "gen_s, crit_s = load(label, GAN_TYPE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "72addddb7ac2720568c6212a7791aadf",
     "grade": false,
     "grade_id": "cell-30ccad6ec04a9284",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Let's plot the target curves, along with some generated examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 3, figsize=(12, 4))\n",
    "\n",
    "noise = trainer.make_noise_input(512)\n",
    "examples = generator(noise)\n",
    "examples = examples.detach().cpu().numpy()\n",
    "ax[0].plot(examples[:, 0], examples[:, 1], \"b.\", alpha=0.9)\n",
    "ax[0].plot(trainer.dataset.means_x, trainer.dataset.means_y, \"r+\")\n",
    "ax[0].set_title(\"Generated samples\")\n",
    "ax[0].set_xlim(-2, 2)\n",
    "ax[0].set_ylim(-2, 2)\n",
    "\n",
    "# plot generator and critic targets, every 500 steps\n",
    "x = np.arange(0, len(critic_targets)) * 500\n",
    "ax[1].plot(x, generator_targets, label=\"generator\")\n",
    "ax[1].legend()\n",
    "ax[1].set_xlabel(\"Training steps\")\n",
    "ax[1].set_ylabel(\"Target\")\n",
    "\n",
    "ax[2].plot(x, critic_targets, label=\"critic\")\n",
    "ax[2].legend()\n",
    "ax[2].set_xlabel(\"Training steps\")\n",
    "ax[2].set_ylabel(\"Target\")\n",
    "plt.savefig(f\"{OUTPUT_DIR}/{GAN_TYPE}_training.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the generated data it is possible that you will see only some of the modes represented, what is commonly called mode collapse. \n",
    "\n",
    "- What are commonly cited reasons for mode collapse?\n",
    "- How does WGAN attempt to avoid mode collapse?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "61d024a3e0772c4cd3e49fe330708014",
     "grade": false,
     "grade_id": "cell-375ebe1d22e3bcee",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "It could potentially be interesting to see how the decision boundary looks. Note that the output is bounded (0,1) when using `GAN_TYPE=\"original\"`, which is not the case for `GAN_TYPE=\"wgan\"` and therefore we normalize the outputs to this range in the latter case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the decision boundary of the discriminator\n",
    "x = np.linspace(-2, 2, 200)\n",
    "y = np.linspace(-2, 2, 200)\n",
    "xx, yy = np.meshgrid(x, y)\n",
    "grid = np.c_[xx.ravel(), yy.ravel()]\n",
    "grid = torch.tensor(grid, dtype=torch.float32).to(DEVICE)\n",
    "trainer.critic.eval()\n",
    "with torch.no_grad():\n",
    "    if GAN_TYPE == \"wgan\":\n",
    "        probs = trainer.critic(grid)[:, 0]\n",
    "        # note that the critic output is not bounded as in the original GAN, so we normalize it to [0, 1]\n",
    "        # to be able to plot it.\n",
    "        probs = (probs - probs.min()) / (probs.max() - probs.min())\n",
    "    elif GAN_TYPE == \"original\":\n",
    "        probs = trainer.critic(grid)\n",
    "        probs = torch.softmax(probs, dim=1)[:, 0]\n",
    "probs = probs.cpu().numpy().reshape(xx.shape)\n",
    "# normalize to [0, 1]\n",
    "\n",
    "# countour plot\n",
    "plt.contourf(xx, yy, probs, 25, cmap=\"RdBu\", vmin=0, vmax=1)\n",
    "plt.colorbar()\n",
    "# scatter plot\n",
    "plt.scatter(examples[:, 0], examples[:, 1], c=\"b\", alpha=0.9)\n",
    "plt.scatter(trainer.dataset.means_x, trainer.dataset.means_y, c=\"r\", alpha=0.9)\n",
    "plt.title(\"Decision boundary\")\n",
    "plt.savefig(f\"{OUTPUT_DIR}/decision_boundary_{GAN_TYPE}.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "73bd105a3366ddce38856b1abcc0ce18",
     "grade": false,
     "grade_id": "cell-ad115ad3d4566c21",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Now, you've seen that changing from the original GAN to the WGAN can make a big difference. Especially, the WGAN is more stable and, in general, produces better results. As you've seen, there are only minor differences between the two implementations. One difference is that we clamp the parameters of the critic `self.critic.apply(clamp_params)` when using the WGAN. \n",
    "\n",
    "Why do we do this?\n",
    "What would happen if we didn't do this?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e1f94582f595e810b7efca5a2d3752d9",
     "grade": true,
     "grade_id": "cell-c15f71a30acf2a3c",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dgm",
   "language": "python",
   "name": "dgm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
