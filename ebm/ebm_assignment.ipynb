{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Energy-Based Models - Home Assignment\n",
    "\n",
    "Welcome to the assignment on Energy-Based Models (EBMs). There are three parts in this assignment:\n",
    "1. You will implement a very simple EBM and train it on a 2D toy dataset using the Maximum Likelihood Estimation (MLE) method with Markov Chain Monte Carlo (MCMC) sampling.\n",
    "2. You will change the training method to Noise Contrastive Estimation (NCE) and its variant conditional NCE (cNCE) and train a model on 2D toy datasets. \n",
    "3. You will train a slightly more complex EBM on the MNIST dataset using Sliced Score Matching (SSM) in another notebook (SSM.ipynb).\n",
    "\n",
    "Throughout this assignment, there are several places where you will need to fill in the code. These are marked with `YOUR CODE HERE` comments. Further, there are several places where you will need to answer questions. These are marked with `YOUR ANSWER HERE` comments. You should replace the `YOUR CODE HERE` and `YOUR ANSWER HERE` comments with your code and answers. \n",
    "\n",
    "### Conda environment\n",
    "You can use the same environment as in the Normalizing Flows assignment. Otherwise, you can create a new environment with the following command:\n",
    "\n",
    "GPU version:\n",
    "```\n",
    "conda env create -f environments/ebm_gpu.yml\n",
    "```\n",
    "CPU version:\n",
    "```\n",
    "conda env create -f environments/ebm_cpu.yml\n",
    "``` \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.autograd as autograd\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "### Device settings\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Device: {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Session 1: MLE with MCMC sampling\n",
    "\n",
    "In this session, you will create a simple MLP that tries to learn a 2D toy dataset containing 8 Gaussian distributions. You will use the MLE method with MCMC sampling to train the model and sample from it. Let's take a look at the dataset first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from toy_data import toy_dataset\n",
    "from toy_data import vis_nce\n",
    "\n",
    "### Feel free to try out other datasets\n",
    "datasets = ['pinwheel', '8gaussians', 'checkerboard', '2spirals', 'rings']\n",
    "\n",
    "### Plot samples from the training set\n",
    "fig, ax = plt.subplots(1, 5, figsize=(20, 4))\n",
    "for i, dataset_name in enumerate(datasets):\n",
    "    X_train = toy_dataset.return_dataset(dataset_name, 1000)[0]\n",
    "    toy_dataset.plot_2d_samples(ax[i], X_train[:][0])\n",
    "    ax[i].set_title(dataset_name)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to define the MLP model. The MLP model should take a 2D input and output a single value, which is the energy of the corresponding input. The architecture is partially defined in the default values of arguments. Specifically, the model should have:\n",
    "- 2 hidden layers with 100 units each,\n",
    "- ReLU activation function for hidden layers,\n",
    "- Linear activation function for the output layer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Define the model\n",
    "class MLP(torch.nn.Module):\n",
    "    def __init__(self, input_dim:int=2, hidden_dims:tuple=(100, 100), output_dim:int=1):\n",
    "        super().__init__()\n",
    "        self.net = torch.nn.Sequential(\n",
    "            ### NOTE: YOUR CODE HERE\n",
    "            nn.Linear(input_dim, hidden_dims[0]),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dims[0], hidden_dims[1]),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dims[1], output_dim)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to right the Langevin sampling method. The accept-reject step is ignored, which means the actual algorithm is the Unjusted Langevin Algorithm (ULA). The algorithm should take the following arguments:\n",
    "- `x0`: the input data as the initial states,\n",
    "- `model`: the MLP model,\n",
    "- `stepsize`: the stepsize of the Langevin dynamics,\n",
    "- `n_steps`: the number of steps of the Langevin dynamics,\n",
    "- `noise_scale`: the scale of the noise added to the Langevin dynamics (you can just use the default)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_langevin(x0: torch.Tensor, model: nn.Module, stepsize: float, n_steps: int, noise_scale:float=None, intermediate_samples=False, inverse=False):\n",
    "    \"\"\"Draw samples using Langevin dynamics\n",
    "    x0: torch.Tensor, initial points\n",
    "    model: An energy-based model\n",
    "    noise_scale: Optional. float. If None, set to np.sqrt(stepsize * 2)\n",
    "    inverse: bool. If True, use the inverse dynamics (the inverse of the gradient)\n",
    "    \"\"\"\n",
    "    if noise_scale is None:\n",
    "        noise_scale = np.sqrt(stepsize * 2)\n",
    "\n",
    "    x = x0\n",
    "    x.requires_grad = True\n",
    "    l_samples = [x.detach().to('cpu')]\n",
    "    l_dynamics = []\n",
    "    for _ in range(n_steps):\n",
    "        noise = torch.randn_like(x) * noise_scale\n",
    "        ### NOTE: YOUR CODE HERE\n",
    "        energy = model(x)\n",
    "        grad = autograd.grad(energy.sum(), x)[0]\n",
    "        dynamics = - stepsize * grad + noise # should consider the inverse dynamics\n",
    "        x = x + dynamics\n",
    "        \n",
    "        l_samples.append(x.detach().to('cpu'))\n",
    "        l_dynamics.append(dynamics.detach().to('cpu'))\n",
    "\n",
    "    if intermediate_samples:\n",
    "        return l_samples, l_dynamics\n",
    "    else:\n",
    "        return l_samples[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can test our Langevin sampler by drawing samples from a Gaussian distribution and checking the intermediate samples and dynamics. We can use the negative log probability as the energy. Do you think the dynamics look reasonable? Compare these two plots and discuss the choice of the stepsize."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`YOUR ANSWER HERE`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_states = torch.randn(50, 2)\n",
    "def langevin_test(init_states, step_size:float=0.01):\n",
    "    def target_two_dim_gaussian():\n",
    "        return torch.distributions.MultivariateNormal(2*torch.ones(2), 0.2*torch.eye(2))\n",
    "\n",
    "    def energy_temp(x):\n",
    "        return - target_two_dim_gaussian().log_prob(x)\n",
    "\n",
    "    real_samples = target_two_dim_gaussian().sample((500,))\n",
    "    l_samples, l_dynamics = sample_langevin(init_states, energy_temp, step_size, 100, intermediate_samples=True)\n",
    "\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(12, 8))\n",
    "    ax_list = axes.flatten()\n",
    "    ckp_list = [1, 5, 10, 20, 40, 60]\n",
    "    for i, ckp in enumerate(ckp_list):\n",
    "        ax_list[i].scatter(real_samples[:, 0], real_samples[:, 1], s=1, c='g', label='Target')\n",
    "        ax_list[i].scatter(l_samples[0][:, 0], l_samples[0][:, 1], s=5, c='b', label='Initial')\n",
    "        ax_list[i].scatter(l_samples[ckp][:, 0], l_samples[ckp][:, 1], s=10, c='r', label='Sampled')\n",
    "        toy_dataset.plot_2d_samples_with_langevin_dynamics(ax_list[i], l_samples[ckp], l_dynamics[ckp-1])\n",
    "        ax_list[i].set_title(f'Step {ckp}')\n",
    "    [ax.axis('equal') for ax in ax_list]\n",
    "    ax_list[0].legend()\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "langevin_test(init_states, step_size=0.01)\n",
    "langevin_test(init_states, step_size=0.02)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have implemented the ULA. We can use it to train the model. One last thing we need to do is to define the loss function. The loss function should take the following arguments:\n",
    "- `energy_positive`: the energy of the positive samples (data),\n",
    "- `energy_negative`: the energy of the negative samples (samples from the model),\n",
    "- `alpha`: the L2 regularization parameter, to limit the energy values.\n",
    "\n",
    "Then, we are ready to train our EBM on the toy dataset \"8gaussians\"!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mle_loss_function(energy_pos, energy_neg, alpha:float=0.1):\n",
    "    ### NOTE: YOUR CODE HERE\n",
    "    reg = alpha * (torch.mean(energy_pos**2) + torch.mean(energy_neg**2))\n",
    "    loss = energy_pos.mean() - energy_neg.mean() + reg\n",
    "    \n",
    "    return loss\n",
    "\n",
    "### Load the training set\n",
    "X_train = toy_dataset.return_dataset('8gaussians', 1000)[0]\n",
    "dl_train = DataLoader(X_train, batch_size=128, shuffle=True, num_workers=8)\n",
    "\n",
    "### Define the training parameters\n",
    "n_epoch = 50     # number of epochs\n",
    "stepsize = 0.1   # Langevin dynamics step size\n",
    "n_step = 100     # The number of Langevin dynamics steps\n",
    "alpha = 0.1      # Regularizer coefficient\n",
    "batch_size = 128 # Batch size\n",
    "\n",
    "model = MLP().to(device)\n",
    "opt = Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "X = torch.randn(1000, 2).to(device)\n",
    "X = sample_langevin(X, model, stepsize, n_step, intermediate_samples=False).to('cpu').detach()\n",
    "toy_dataset.vis_result(X_train, X, model, device=device)\n",
    "\n",
    "for i_epoch in range(n_epoch):\n",
    "        l_loss = []\n",
    "        for pos_x, in dl_train:\n",
    "            \n",
    "            pos_x:torch.Tensor = pos_x.to(device)\n",
    "\n",
    "            ### NOTE: YOUR CODE HERE\n",
    "            x_0 = torch.randn(X.size())\n",
    "            # No grad. from the sampling\n",
    "            neg_x = sample_langevin(x_0, model, stepsize, n_step, intermediate_samples=False).to(device).detach()\n",
    "            pos_out = model(pos_x)\n",
    "            neg_out = model(neg_x)\n",
    "            loss = mle_loss_function(pos_out, neg_out, alpha)\n",
    "\n",
    "            opt.zero_grad()\n",
    "            loss.backward()\n",
    "            \n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.1)\n",
    "            opt.step()\n",
    "            \n",
    "            l_loss.append(loss.item())\n",
    "        print(f'Epoch {i_epoch+1}/{n_epoch}. Mean loss {round(np.mean(l_loss), 4)}')\n",
    "\n",
    "        ### Visualize the results every 10 epochs\n",
    "        if ((i_epoch+1) % 10 == 0):\n",
    "            X = torch.randn(1000, 2).to(device)\n",
    "            X = sample_langevin(X, model, stepsize, n_step, intermediate_samples=False).to('cpu').detach()\n",
    "            toy_dataset.vis_result(X_train, X, model, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Session 2: NCE and cNCE\n",
    "\n",
    "Now we switch to different ways to train our EBMs. First of all, we will use the NCE method to train our EBM, for which we need to use a different network architecture. The difference here is that we need to model the partition function as a learnable parameter. Feel free to use any architecture you like. Why is it important to model the partition function as a learnable parameter?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because the NCE criterion includes evaluations of $p_\\theta(\\cdot)$, which requires an explicitly normalised distribution. For the ML-estimaion above, we got around the issue of the normalisation by estimating $Z_\\theta$ from MCMC-samples. Later, with the CNCE criterion, we will see that we only need to evaluate the model distribution up to a constant (i.e. we only need to be able to compute the energy). But for the standard NCE, we need an explicit normalisation.\n",
    "\n",
    "The glass half full interpretation is that NCE allows us to learn a normalised distribution directly. The glass half empty version is that this is very limiting, consider a simple extension where we want to model a conditional distribution $p_\\theta(x \\mid y)$, with $y \\in \\mathbb{R}$ a simple real scalar. Then $Z_\\theta(y)$ is a function of $y$ and this learnable parameter trick will not work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPC(nn.Module):\n",
    "    def __init__(self, x_dim):\n",
    "        \"\"\"\n",
    "        Initialize EBM model, which is an MLP with additional estimated partition function c.\n",
    "\n",
    "        Args:\n",
    "            x_dim (int): The size of the input data.\n",
    "        \"\"\"\n",
    "\n",
    "        super(MLPC, self).__init__()\n",
    "        ### NOTE: YOUR CODE HERE\n",
    "        # Re-use the MCMC-ML model for a somewhat fair comparison\n",
    "        hidden_dims = (100, 100)\n",
    "        input_dim = x_dim\n",
    "        self.energy = torch.nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dims[0]),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dims[0], hidden_dims[1]),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dims[1], 1)\n",
    "        )\n",
    "        self.log_z = nn.Parameter(torch.randn((1)))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"Compute log p_theta(x)\"\"\"\n",
    "        # NOTE YOUR CODE HERE\n",
    "        return - self.energy(x) - self.log_z\n",
    "\n",
    "    \n",
    "### Test the model\n",
    "batch_size = 16\n",
    "x_dim = 10\n",
    "x_rand = torch.randn((batch_size, x_dim))\n",
    "ebm = MLPC(x_dim)\n",
    "assert ebm(x_rand).shape == (batch_size, 1), \"Generator output shape is wrong\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define a noise distribution. Write code for your noise distribution. Could be any distribution of your choice. Looking into `torch.distributions` could be a good idea. The argument `params` depends on which distribution you chose. First recall what should be considered when selecting an appropriate noise distribution:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are very few formal requirements on the noise distribution $p_n$. We do need a certain level of support:\n",
    "$$\n",
    "p_n(x) > 0\\; \\forall x: p_d(x) > 0.\n",
    "$$\n",
    "Beyond that, to use the NCE crit. we need $p_n$ to be relatively simple to sample from and to evaluate the pdf exactly. Ideally, it should also be close to the model distribution $p_\\theta$, but this is tricky to achieve without having learnable/adaptive noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.distributions.multivariate_normal import MultivariateNormal\n",
    "from torch.distributions.categorical import Categorical\n",
    "\n",
    "class Gaussian:\n",
    "    def __init__(self, x_dim, params) -> None:\n",
    "        # NOTE YOUR CODE HERE\n",
    "        mean = torch.zeros((x_dim,))\n",
    "        scale = params\n",
    "        cov = torch.eye(x_dim)\n",
    "        self.distribution = MultivariateNormal(loc=mean, covariance_matrix=cov)\n",
    "        \n",
    "    def sample(self, n_samples):\n",
    "        '''\n",
    "        returns n_samples from the distribution\n",
    "        '''\n",
    "        return self.distribution.sample((n_samples,))\n",
    "        \n",
    "    \n",
    "    def log_prob(self, x):\n",
    "        '''\n",
    "        returns the log_probability of a batch of samples\n",
    "        '''\n",
    "        return self.distribution.log_prob(x).reshape((x.size(0), 1))\n",
    "\n",
    "### Look at your noise\n",
    "batch_size = 10000\n",
    "x_dim = 2\n",
    "distribution_params = [4] # TODO, depends on which distribution you use.\n",
    "plot_lim = 4 # for plot, may need to be adjusted depending on your noise dist parameters\n",
    "n_pts = 700 # for plot 2D hist bins\n",
    "noise_dist = Gaussian(x_dim, distribution_params)\n",
    "noise_samples = noise_dist.sample(batch_size)\n",
    "\n",
    "assert noise_samples.shape == (batch_size, x_dim), \"Noise distribution samples shapes are wrong\"\n",
    "\n",
    "log_probs = noise_dist.log_prob(noise_samples)\n",
    "assert log_probs.shape == (batch_size, 1), \"Noise distribution samples shapes are wrong\"\n",
    "\n",
    "# plot samples and pdf\n",
    "fig, axs = plt.subplots(1, 2, figsize=(12,4.3), subplot_kw={'aspect': 'equal'})\n",
    "vis_nce.plot_samples(noise_samples, axs[0], plot_lim, n_pts)\n",
    "axs[0].set_title(f'{batch_size} noise samples')\n",
    "test_grid = vis_nce.setup_grid(plot_lim, n_pts, device)\n",
    "vis_nce.plot_noise(noise_dist, axs[1], test_grid, n_pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have coded an EBM model and a noise distribution of your choice, it is time to code the noise contrastive estimator (NCE). \n",
    "\n",
    "Describe the idea of the NCE and why it is useful. \n",
    "What is the purpose of using the parameter `k`?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`YOUR ANSWER HERE`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NCE:\n",
    "    def __init__(self, ebm, noise_dist, k) -> None:\n",
    "        \"\"\"\n",
    "        Initializes the NCE\n",
    "        \n",
    "        Args:\n",
    "        ebm (EBM): The energy based model.\n",
    "        noise_dist (torch.distributions.distribution): The noise distribution. Could be from torch.distributions. Can also be some custom distribution.\n",
    "        k (int): The nr of noise data points for each target data point. \\nu in the slides from lecture.\n",
    "        \"\"\"\n",
    "        self.ebm = ebm\n",
    "        self.noise_dist = noise_dist \n",
    "        self.k = k\n",
    "    \n",
    "    def loss(self, x) -> torch.Tensor:\n",
    "        '''\n",
    "        returns the NCE-loss given a batch x from the dataset\n",
    "        '''\n",
    "        # NOTE YOUR CODE HERE\n",
    "        p_theta_x = torch.exp(self.ebm(x))\n",
    "        q_x = torch.exp(self.noise_dist.log_prob(x))\n",
    "        pos_den = p_theta_x + self.k * q_x\n",
    "        pos_term = torch.log(p_theta_x) - torch.log(pos_den)\n",
    "        \n",
    "        y = self.noise_dist.sample(x.shape[0] * self.k)\n",
    "        p_theta_y = torch.exp(self.ebm(y))\n",
    "        q_y = torch.exp(self.noise_dist.log_prob(y))\n",
    "        neg_den = p_theta_y + self.k * q_y\n",
    "        neg_term = torch.log(self.k * q_y) - torch.log(neg_den)\n",
    "        return torch.mean(pos_term) + self.k * torch.mean(neg_term)    \n",
    "\n",
    "### Test the NCE loss\n",
    "x_dim = 2\n",
    "x_rand = torch.randn((batch_size, x_dim)).to(device)\n",
    "k = 5\n",
    "distribution_params = [4] # TODO, depends on which distribution you use.\n",
    "\n",
    "ebm = MLPC(x_dim=x_dim).to(device)\n",
    "noise_dist = Gaussian(x_dim, distribution_params)\n",
    "nce = NCE(ebm, noise_dist, k)\n",
    "loss = nce.loss(x_rand)\n",
    "# Assert scalar\n",
    "assert loss.shape == torch.Size([]), \"Loss shape is wrong\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch, train_loader, ebm, optimizer, nce):\n",
    "    ebm.train()\n",
    "    train_loss = 0\n",
    "    for batch_idx, x in enumerate(train_loader):\n",
    "        x = x.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loss = nce.loss(x)\n",
    "        loss.backward()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch_idx % 10 == 0:\n",
    "            print(\n",
    "                \"Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\".format(\n",
    "                    epoch,\n",
    "                    batch_idx * len(x),\n",
    "                    len(train_loader.dataset),\n",
    "                    100.0 * batch_idx / len(train_loader),\n",
    "                    loss.item() / len(x),\n",
    "                )\n",
    "            )\n",
    "    print(\n",
    "        \"====> Epoch: {} Average loss: {:.4f}\".format(\n",
    "            epoch, train_loss / len(train_loader.dataset)\n",
    "        )\n",
    "    )\n",
    "    return train_loss / len(train_loader.dataset)\n",
    "\n",
    "def test(test_loader, ebm, nce):\n",
    "    ebm.eval()\n",
    "    test_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for x in test_loader:\n",
    "            x = x.to(device)\n",
    "\n",
    "            # sum up batch loss\n",
    "            test_loss += nce.loss(x).item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print(\"====> Test set loss: {:.4f}\".format(test_loss))\n",
    "    return test_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below will train an EBM with NCE on the selected generated dataset.\n",
    "\n",
    "Feel free to explore the usage of the different datasets available and how the learning differs for different values of k and the noise distribution parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify parameters\n",
    "n_epochs = 10\n",
    "batch_size = 128\n",
    "train_size = 0.8\n",
    "n_samples = 10_000\n",
    "distribution_params = [4]\n",
    "k = 5\n",
    "x_dim = 2\n",
    "training_losses = np.zeros([n_epochs, 1])\n",
    "test_losses = np.zeros([n_epochs, 1])\n",
    "dataset_name = 'pinwheel' # one of '8gaussians', 'checkerboard', '2spirals', 'pinwheel'\n",
    "results_dir = f'results/NCE/{dataset_name}'\n",
    "\n",
    "os.makedirs(results_dir, exist_ok=True)\n",
    "os.makedirs('models', exist_ok=True)\n",
    "\n",
    "# Load generated data and create dataloaders\n",
    "train_dataset = toy_dataset.return_dataset(dataset_name, int(train_size*n_samples))[0][:][0]\n",
    "test_dataset  = toy_dataset.return_dataset(dataset_name, n_samples-int(train_size*n_samples))[0][:][0]\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=8)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True, num_workers=8)\n",
    "\n",
    "# Build model\n",
    "ebm = MLPC(x_dim).to(device)\n",
    "# Use defined noise dist\n",
    "noise_dist = Gaussian(x_dim, distribution_params)\n",
    "# Build NCE\n",
    "nce = NCE(ebm, noise_dist, k)\n",
    "optimizer = Adam(ebm.parameters())\n",
    "\n",
    "# Perform training\n",
    "start_time = time.time()\n",
    "best_loss = torch.inf\n",
    "for epoch in range(1, n_epochs):\n",
    "    train_loss = train(epoch, train_dataloader, ebm, optimizer, nce)\n",
    "    test_loss = test(test_dataloader, ebm, nce)\n",
    "    if test_loss < best_loss:\n",
    "        torch.save(ebm, 'models/nce.pt')\n",
    "        best_loss = test_loss\n",
    "    training_losses[epoch - 1] = train_loss\n",
    "    test_losses[epoch - 1] = test_loss\n",
    "    \n",
    "    vis_nce.plot_nce(train_dataset, ebm, noise_dist, device, os.path.join(results_dir, f'epoch_{epoch}.png'))\n",
    "\n",
    "end_time = time.time()\n",
    "time_elapsed = end_time - start_time\n",
    "minutes, seconds = divmod(time_elapsed, 60)\n",
    "print(\"Time elapsed during training: %d minutes and %d seconds\" % (minutes, seconds))\n",
    "\n",
    "# Plot training and test losses\n",
    "plt.plot(range(1, n_epochs), training_losses[:-1])\n",
    "plt.plot(range(1, n_epochs), test_losses[:-1])\n",
    "plt.grid()\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend([\"Training Loss\", \"Test Loss\"])\n",
    "plt.savefig(os.path.join(results_dir, 'train_test_loss.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ebm(torch.randn(5,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to also implement the conditional NCE (cNCE). What is considered the advantage of cNCE over NCE? Are there any potential disadvantages?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`YOUR ANSWER HERE`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class cNCE:\n",
    "    def __init__(self, ebm) -> None:\n",
    "        \"\"\"\n",
    "        Initializes the conditional NCE\n",
    "        \n",
    "        Args:\n",
    "        ebm (EBM): The energy based model.\n",
    "        noise_dist (torch.distributions.distribution): The noise distribution. Could be from torch.distributions. Can also be some custom distribution.\n",
    "        \"\"\"\n",
    "        self.ebm = ebm\n",
    "    \n",
    "    def add_noise(self, x):\n",
    "        '''\n",
    "        returns the noised batch x\n",
    "        '''\n",
    "        # NOTE YOUR CODE HERE\n",
    "        std = 1/2\n",
    "        eps = torch.randn_like(x)\n",
    "        return x + std * eps\n",
    "    \n",
    "    def loss(self, x) -> torch.Tensor:\n",
    "        '''\n",
    "        returns the cNCE-loss given a batch x from the dataset\n",
    "        '''\n",
    "        # NOTE YOUR CODE HERE\n",
    "        x_noise = self.add_noise(x)\n",
    "        log_p_theta_x = self.ebm(x)\n",
    "        log_p_theta_x_noise = self.ebm(x_noise)\n",
    "        tmp = torch.column_stack((log_p_theta_x, log_p_theta_x_noise))        \n",
    "        loss = log_p_theta_x - torch.logsumexp(tmp, dim=1)\n",
    "\n",
    "        return -loss.mean()        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify parameters\n",
    "n_epochs = 10\n",
    "batch_size = 128\n",
    "train_size = 0.8\n",
    "n_samples = 10_000\n",
    "x_dim = 2\n",
    "training_losses = np.zeros([n_epochs, 1])\n",
    "test_losses = np.zeros([n_epochs, 1])\n",
    "dataset_name = 'pinwheel' # one of '8gaussians', 'checkerboard', '2spirals', 'pinwheel'\n",
    "results_dir = f'results/cNCE/{dataset_name}'\n",
    "\n",
    "os.makedirs(results_dir, exist_ok=True)\n",
    "os.makedirs('models', exist_ok=True)\n",
    "\n",
    "# Load generated data and create dataloaders\n",
    "train_dataset = toy_dataset.return_dataset(dataset_name, int(train_size*n_samples))[0][:][0]\n",
    "test_dataset  = toy_dataset.return_dataset(dataset_name, n_samples-int(train_size*n_samples))[0][:][0]\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=8)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True, num_workers=8)\n",
    "\n",
    "# Build model\n",
    "ebm = MLPC(x_dim).to(device)\n",
    "# Build cNCE\n",
    "cnce = cNCE(ebm)\n",
    "optimizer = Adam(ebm.parameters())\n",
    "\n",
    "# Perform training\n",
    "start_time = time.time()\n",
    "best_loss = torch.inf\n",
    "for epoch in range(1, n_epochs):\n",
    "    train_loss = train(epoch, train_dataloader, ebm, optimizer, cnce)\n",
    "    test_loss = test(test_dataloader, ebm, cnce)\n",
    "    if test_loss < best_loss:\n",
    "        torch.save(ebm, 'models/cnce.pt')\n",
    "        best_loss = test_loss\n",
    "    training_losses[epoch - 1] = train_loss\n",
    "    test_losses[epoch - 1] = test_loss\n",
    "    \n",
    "    # had to use cpu on my machine to plot, otherwise weird cuda error.\n",
    "    vis_nce.plot_cnce(train_dataset, ebm, cnce, device, os.path.join(results_dir, f'epoch_{epoch}.png'))\n",
    "\n",
    "end_time = time.time()\n",
    "time_elapsed = end_time - start_time\n",
    "minutes, seconds = divmod(time_elapsed, 60)\n",
    "print(\"Time elapsed during training: %d minutes and %d seconds\" % (minutes, seconds))\n",
    "\n",
    "# Plot training and test losses\n",
    "plt.plot(range(1, n_epochs), training_losses[:-1])\n",
    "plt.plot(range(1, n_epochs), test_losses[:-1])\n",
    "plt.grid()\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend([\"Training Loss\", \"Test Loss\"])\n",
    "plt.savefig(os.path.join(results_dir, 'train_test_loss.png'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It may be observed that the cNCE is harder to train than the NCE. What are your observations when comparing training with NCE and cNCE?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`YOUR ANSWER HERE`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have trained an EBM model via NCE, we want to sample from the trained EBM. Reuse implemented Langevin dynamics to do this. Consider the limitations of Langevin dynamics in your attempt to sample the target distribution. Chose the dataset you want and either the saved NCE or cNCE model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step_size = 0.01\n",
    "n_initializations = 10\n",
    "steps = 3000\n",
    "\n",
    "ebm = torch.load('models/nce.pt').to(device)\n",
    "fig, axs = plt.subplots(1, 2, figsize=(12,4.3), subplot_kw={'aspect': 'equal'})\n",
    "samples = []\n",
    "for j in range(n_initializations):\n",
    "    x = 3*torch.randn(2).to(device)\n",
    "    xs, _ = sample_langevin(x, ebm, step_size, steps, intermediate_samples=True, inverse=True)\n",
    "    samples += [x_.detach().cpu().numpy() for x_ in xs]\n",
    "\n",
    "samples = torch.tensor(np.array(samples))\n",
    "\n",
    "n_pts = 700\n",
    "range_lim = 4\n",
    "\n",
    "xx, yy, zz = vis_nce.setup_grid(range_lim, n_pts, device)\n",
    "log_prob = ebm.to('cpu')(zz.to('cpu')).detach()\n",
    "prob = log_prob.exp().cpu()\n",
    "# plot\n",
    "vis_nce.plot_samples(samples, axs[0], range_lim, n_pts)\n",
    "axs[0].set_title('Langevin samples')\n",
    "\n",
    "axs[1].pcolormesh(xx, yy, prob.view(n_pts,n_pts), cmap=plt.cm.jet)\n",
    "axs[1].set_facecolor(plt.cm.jet(0.))\n",
    "axs[1].set_title('Energy density')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "normflow_gpu",
   "language": "python",
   "name": "normflow_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
